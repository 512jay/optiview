Hereâ€™s your updated `.dev_notes.md` file with the latest changes included:

---

```markdown
# ğŸ› ï¸ OptiView Development Notes

This file tracks current project structure, completed phases, initialization procedures, and future enhancements. Keep it up to date as features are built.

---

## âœ… Current Progress

### Phase 1: Data Ingestion
- [x] Custom XML parser for MT5 reports
- [x] Fully type-safe ingestion logic with dynamic schema mapping
- [x] Properly stores float parameter values for input configs

### Phase 2: Exploratory Data Analysis (EDA)
- [x] Streamlit-based histogram and scatterplot visualizations
- [x] Correlation matrix and boxplot support
- [x] Parameter stability identification
- [x] Trade count filtering toggle added

### Phase 3: Baseline ML Model
- [x] `predict_optimal_config()` (formerly `predict_next_month`) added
- [x] Uses past month(s) of runs to predict best config for next
- [x] Supports both `custom_score` and `profit` as targets
- [x] Models supported:
  - `xgb` (XGBoost)
  - `rf` (Random Forest)
  - `gbr` (Gradient Boosting)
  - `histgb` (HistGradientBoosting)
  - `lgbm` (LightGBM)
  - `cat` (CatBoost)
- [x] Dynamic model selection via CLI or override

### Phase 4: Walk-Forward Prediction Engine
- [x] `bulk_predict.py` now loops over all months/symbols/models
- [x] Stores predictions in `generated/predictions/{YYYY-MM}/{SYMBOL}/{MODEL}/prediction_summary.csv`
- [x] Optionally filters training data
- [x] Confidence scores added via quality scoring engine

### Phase 5: Interactive Analysis Tool (UI)
- [x] Streamlit sidebar with dropdowns: month, symbol, target
- [x] "Run Prediction" triggers `predict_optimal_config()` and saves results
- [x] View tab displays ranked predictions and confidence stars
- [x] Predictions include model, date, inputs, and expected profit

---

# FULL RESET:
rm -rf generated

# 1. Predict for all symbols and models
poetry run python src/optiview/engine/walk_forward/bulk_predict.py

# 2. Annotate predictions with actuals
poetry run python src/optiview/engine/walk_forward/annotate_predictions.py

```

---

## ğŸ—ƒï¸ Tabled Ideas

- [ ] Add support for exporting quality scores into each prediction folder (done partially)
- [ ] Model leaderboard comparing historical performance
- [ ] Summary report per `bulk_predict` run (e.g. failures, skips)
- [ ] Config Quality Determiner: summarize each month's model confidence
- [ ] Remove filters entirely and rely on predictive confidence instead
- [ ] Add "Initialize System" button to Streamlit app
- [ ] Improve Streamlit UX to auto-jump to results tab after prediction
- [ ] Better visualizations for actual vs predicted metrics
- [ ] Integrate live testing & deployment prep

---

## ğŸ§© Streamlit UI Layout Plan

### ğŸ”¹ Start Page: OptiView Overview
- [x] "Initialize System" button (deletes `generated/`, runs predictions and annotation)
- [ ] Display last prediction/annotation status and model coverage
- [ ] Dev tools / debug info (hidden or collapsible)

### ğŸ”¹ Page 1: View Predictions
- [x] Dropdowns: Symbol, Month
- [x] Table of predictions from `prediction_summary.csv`
- [x] Confidence stars, actual vs predicted profit
- [ ] Add toggle for viewing all_models.csv comparison
- [ ] Filter by model or quality_score threshold

### ğŸ”¹ Page 2: Run Predictions
- [x] Dropdowns: Symbol, Month, Target
- [x] "Run Prediction" button executes `predict_optimal_config()`
- [x] Toast message shows success and links to "View Predictions"
- [ ] Option: Overwrite or skip existing predictions

### ğŸ”¹ Page 3: Model Leaderboard (ğŸ“Š Tabled)
- [ ] Show average quality scores by model across all months
- [ ] Plot RMSE, hit rate, ranking accuracy
- [ ] Add "drill-down" per month/model

---

## ğŸ–¼ï¸ UI Page-to-Function Mapping (Sketch)

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ğŸ  OptiView         â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  ğŸ“Š View Predictions       â”‚  <- 01_View_Predictions.py
â”‚  ğŸ§  Run Predictions        â”‚  <- 03_Run_Predictions.py
â”‚  ğŸ§ª Initialize System      â”‚  <- OptiView.py
â”‚  ğŸ† Model Leaderboard (â¸)  â”‚  <- Tabled future
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## ğŸ§  Notes for New Devs or ML Beginners

- A **baseline ML model** learns how past input parameters affect future trading performance. Itâ€™s your first pass at automation.
- `custom_score = profit - 2 * drawdown` (useful for penalizing volatile configs)
- XGBoost and Random Forest are good starting models. Others (like LightGBM, CatBoost) offer better performance in some cases.
- Quality scores reflect whether predicted configs from month N were profitable in month N+1.
- Confidence stars shown in UI are based on past predictive accuracy â€” not just predicted score.

---

poetry run python src/optiview/engine/walk_forward/annotate_quality_scores.py

```

Useful commands
streamlit run src/optiview/ui/Optiview.py
poetry run python src/optiview/engine/walk_forward/bulk_predict.py --overwrite
poetry run python src/optiview/engine/walk_forward/annotate_from_database_parquet.py
