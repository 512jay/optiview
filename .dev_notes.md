Here‚Äôs your updated `.dev_notes.md` file with the latest changes included:

---

```markdown
# üõ†Ô∏è OptiView Development Notes

This file tracks current project structure, completed phases, initialization procedures, and future enhancements. Keep it up to date as features are built.

---

## ‚úÖ Current Progress

### Phase 1: Data Ingestion
- [x] Custom XML parser for MT5 reports
- [x] Fully type-safe ingestion logic with dynamic schema mapping
- [x] Properly stores float parameter values for input configs

### Phase 2: Exploratory Data Analysis (EDA)
- [x] Streamlit-based histogram and scatterplot visualizations
- [x] Correlation matrix and boxplot support
- [x] Parameter stability identification
- [x] Trade count filtering toggle added

### Phase 3: Baseline ML Model
- [x] `predict_optimal_config()` (formerly `predict_next_month`) added
- [x] Uses past month(s) of runs to predict best config for next
- [x] Supports both `custom_score` and `profit` as targets
- [x] Models supported:
  - `xgb` (XGBoost)
  - `rf` (Random Forest)
  - `gbr` (Gradient Boosting)
  - `histgb` (HistGradientBoosting)
  - `lgbm` (LightGBM)
  - `cat` (CatBoost)
- [x] Dynamic model selection via CLI or override

### Phase 4: Walk-Forward Prediction Engine
- [x] `bulk_predict.py` now loops over all months/symbols/models
- [x] Stores predictions in `generated/predictions/{YYYY-MM}/{SYMBOL}/{MODEL}/prediction_summary.csv`
- [x] Optionally filters training data
- [x] Confidence scores added via quality scoring engine

### Phase 5: Interactive Analysis Tool (UI)
- [x] Streamlit sidebar with dropdowns: month, symbol, target
- [x] "Run Prediction" triggers `predict_optimal_config()` and saves results
- [x] View tab displays ranked predictions and confidence stars
- [x] Predictions include model, date, inputs, and expected profit

---

## üß™ Initialization Tasks (One-Time or Reset)

When adding new data or refreshing predictions:

```bash
rm -rf generated

# Step 1: Generate predictions for all models/months
poetry run python src/optiview/engine/walk_forward/bulk_predict.py

# Step 2: Evaluate predictive quality
poetry run python src/optiview/analysis/evaluate_predictions.py
```

---

## üóÉÔ∏è Tabled Ideas

- [ ] Add support for exporting quality scores into each prediction folder (done partially)
- [ ] Model leaderboard comparing historical performance
- [ ] Summary report per `bulk_predict` run (e.g. failures, skips)
- [ ] Config Quality Determiner: summarize each month's model confidence
- [ ] Remove filters entirely and rely on predictive confidence instead
- [ ] Add "Initialize System" button to Streamlit app
- [ ] Improve Streamlit UX to auto-jump to results tab after prediction
- [ ] Better visualizations for actual vs predicted metrics
- [ ] Integrate live testing & deployment prep

---

## üß† Notes for New Devs or ML Beginners

- A **baseline ML model** learns how past input parameters affect future trading performance. It‚Äôs your first pass at automation.
- `custom_score = profit - 2 * drawdown` (useful for penalizing volatile configs)
- XGBoost and Random Forest are good starting models. Others (like LightGBM, CatBoost) offer better performance in some cases.
- Quality scores reflect whether predicted configs from month N were profitable in month N+1.
- Confidence stars shown in UI are based on past predictive accuracy ‚Äî not just predicted score.

---

poetry run python src/optiview/engine/walk_forward/annotate_quality_scores.py

```